{
    "www.infobae.com": "from bs4 import BeautifulSoup\nfrom urllib.parse import urljoin\n\ndef scrape_news(html_content):\n    \"\"\"\n    Parses the HTML content of a news page to extract articles.\n\n    Args:\n        html_content (str): The HTML content of the news page.\n\n    Returns:\n        list: A list of dictionaries, where each dictionary represents an article\n              and contains 'title', 'link', and 'description'.\n    \"\"\"\n    BASE_URL = 'https://www.infobae.com/america/'\n    soup = BeautifulSoup(html_content, 'html.parser')\n    news_list = []\n\n    # Selectors based on the provided analysis\n    best_article_selector = \"a.story-card-ctn\"\n    title_selector = \"h2.story-card-hl\"\n    link_selector = \"self\"  # The article container itself is the link tag\n    description_selector = \"h3.story-card-deck\"\n\n    articles = soup.select(best_article_selector)\n\n    for article in articles:\n        try:\n            # Extract title\n            title_element = article.select_one(title_selector)\n            title = title_element.get_text(strip=True) if title_element else None\n\n            # Extract link\n            # 'self' means the article element itself is the anchor tag\n            relative_link = article.get('href')\n            link = urljoin(BASE_URL, relative_link) if relative_link else None\n\n            # Extract description\n            description_element = article.select_one(description_selector)\n            description = description_element.get_text(strip=True) if description_element else \"\"\n\n            # Ensure essential data (title and link) is present\n            if title and link:\n                news_list.append({\n                    'title': title,\n                    'link': link,\n                    'description': description\n                })\n        except Exception:\n            # If any error occurs during processing an article, skip it and continue\n            continue\n            \n    return news_list",
    "www.dw.com": "from bs4 import BeautifulSoup\nfrom urllib.parse import urljoin\n\ndef scrape_news(html_content):\n    \"\"\"\n    Parses HTML content to extract news articles.\n\n    Args:\n        html_content (str): The HTML content of the news page.\n\n    Returns:\n        list: A list of dictionaries, where each dictionary represents a news\n              article with \"title\", \"link\", and \"description\".\n    \"\"\"\n    base_url = \"https://www.dw.com/es/actualidad/s-30684\"\n    soup = BeautifulSoup(html_content, 'html.parser')\n    articles = []\n\n    article_selectors = \".teaser-wrap, .news-item\"\n    title_link_selectors = \".title a, .news-title a\"\n    description_selector = \".teaser-description a\"\n\n    for item in soup.select(article_selectors):\n        try:\n            title = None\n            link = None\n            description = \"\"\n\n            # Extract title and link\n            title_link_element = item.select_one(title_link_selectors)\n            if title_link_element:\n                title = title_link_element.get_text(strip=True)\n                href = title_link_element.get('href')\n                if href:\n                    link = urljoin(base_url, href)\n\n            # Extract description\n            description_element = item.select_one(description_selector)\n            if description_element:\n                description = description_element.get_text(strip=True)\n\n            # Validate that minimum data was found\n            if title and link:\n                articles.append({\n                    \"title\": title,\n                    \"link\": link,\n                    \"description\": description\n                })\n\n        except Exception as e:\n            # If an error occurs for one article, skip it and continue\n            # print(f\"Skipping an article due to an error: {e}\")\n            continue\n\n    return articles",
    "www.youtube.com": "from bs4 import BeautifulSoup\nfrom urllib.parse import urljoin\n\ndef scrape_news(html_content):\n    \"\"\"\n    Scrapes news articles from the provided HTML content based on a predefined analysis.\n\n    Args:\n        html_content (str): The HTML content of the news search results page.\n\n    Returns:\n        list: A list of dictionaries, where each dictionary represents an article\n              and contains \"title\", \"link\", and \"description\".\n    \"\"\"\n    base_url = \"https://www.youtube.com/results?search_query=Am%C3%A9rica+Latina&sp=CAISBBABGAI%253D\"\n    soup = BeautifulSoup(html_content, 'html.parser')\n    \n    articles_data = []\n    \n    # Selectors from the provided analysis\n    best_article_selector = \"ytd-video-renderer\"\n    title_selector = \"a#video-title\"\n    link_selector = \"a#video-title\"\n    description_selector = \"yt-formatted-string.metadata-snippet-text, yt-formatted-string#description-text\"\n    \n    articles = soup.select(best_article_selector)\n    \n    for article in articles:\n        try:\n            # Extract title\n            title_element = article.select_one(title_selector)\n            title = title_element.get_text(strip=True) if title_element else None\n            \n            # Extract link\n            link_element = article.select_one(link_selector)\n            if link_element and link_element.has_attr('href'):\n                relative_url = link_element['href']\n                link = urljoin(base_url, relative_url)\n            else:\n                link = None\n            \n            # Extract description\n            description_element = article.select_one(description_selector)\n            description = description_element.get_text(strip=True) if description_element else \"\"\n            \n            # Validate that minimum data (title and link) was extracted\n            if title and link:\n                articles_data.append({\n                    \"title\": title,\n                    \"link\": link,\n                    \"description\": description\n                })\n                \n        except Exception:\n            # If an error occurs for one article, continue to the next\n            continue\n            \n    return articles_data",
    "www.france24.com": "from bs4 import BeautifulSoup\nfrom urllib.parse import urljoin\n\ndef scrape_news(html_content):\n    \"\"\"\n    Parses HTML content from france24.com/es/ to extract news articles based on predefined selectors.\n\n    Args:\n        html_content (str): The HTML content of the news page as a string.\n\n    Returns:\n        list: A list of dictionaries, where each dictionary represents a news article\n              and contains 'title', 'link', and 'description'. Returns an empty list\n              if no articles are found or in case of an error.\n    \"\"\"\n    BASE_URL = \"https://www.france24.com/es/\"\n    news_list = []\n    \n    try:\n        soup = BeautifulSoup(html_content, 'html.parser')\n        articles = soup.select(\".m-item-list-article\")\n\n        for article in articles:\n            try:\n                title_element = article.select_one(\".article__title h2\")\n                title = title_element.get_text(strip=True) if title_element else None\n\n                link_element = article.select_one(\".article__title a\")\n                relative_link = link_element['href'] if link_element and link_element.has_attr('href') else None\n                \n                description_element = article.select_one(\".article__chapo\")\n                description = description_element.get_text(strip=True) if description_element else None\n\n                if title and relative_link:\n                    link = urljoin(BASE_URL, relative_link)\n                    news_item = {\n                        'title': title,\n                        'link': link,\n                        'description': description\n                    }\n                    news_list.append(news_item)\n\n            except Exception:\n                continue\n                \n    except Exception:\n        # This outer try-except handles potential errors with BeautifulSoup initialization\n        # or the initial `soup.select`, returning an empty list in such cases.\n        return []\n\n    return news_list"
}