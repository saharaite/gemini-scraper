# gemini_scraper_app_final.py

import streamlit as st
import requests
from bs4 import BeautifulSoup
import json
import google.generativeai as genai
from urllib.parse import urlparse
import numpy as np
import time
import os

# ØªÙ„Ø§Ø´ Ø¨Ø±Ø§ÛŒ ÙˆØ§Ø±Ø¯ Ú©Ø±Ø¯Ù† Selenium
try:
    from selenium import webdriver
    from selenium.webdriver.chrome.service import Service as ChromeService
    from selenium.webdriver.chrome.options import Options
    from webdriver_manager.chrome import ChromeDriverManager
    SELENIUM_AVAILABLE = True
except ImportError:
    SELENIUM_AVAILABLE = False

# --- Ø¨Ø®Ø´ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ùˆ Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ ---
st.set_page_config(layout="wide", page_title="ØªØ­Ù„ÛŒÙ„Ú¯Ø± Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø§Ø®Ø¨Ø§Ø± Ø¨Ø§ Gemini")
SCRAPER_FILE = 'scrapers.json'

# --- Ø§Ø³ØªØ§ÛŒÙ„ CSS Ø¨Ø§ ÙÙˆÙ†Øª Ø´Ø¨Ù†Ù… Ø§Ø² CDN Ù…Ø¹ØªØ¨Ø± ---
st.markdown(
    """
    <style>
    @import url('https://cdnjs.cloudflare.com/ajax/libs/shabnam-font/5.0.1/font-face.css');
    
    html, body, [class*="css"]  {
        direction: rtl;
        font-family: 'Shabnam', sans-serif !important;
        font-weight: normal;
        text-align: right;
    }
    h1, h2, h3, h4, h5, h6, strong {
        font-weight: bold !important;
    }
    textarea[key="editor"], code, .stDataFrame {
        direction: ltr !important;
        text-align: left !important;
        font-family: monospace !important;
    }
    .stChatMessage div[data-testid="stMarkdownContainer"] p,
    .stChatMessage div[data-testid="stMarkdownContainer"] ul,
    .stChatMessage div[data-testid="stMarkdownContainer"] ol {
        text-align: right !important;
        direction: rtl !important;
    }
    .stRadio > div {
        flex-direction: row-reverse;
        justify-content: flex-end;
    }
    .stRadio label {
        margin-left: 0.5rem;
        margin-right: 0;
    }
    [data-testid="stSidebar"] {
        text-align: right !important;
    }
    </style>
    """,
    unsafe_allow_html=True
)

# --- Ù…Ø¯ÛŒØ±ÛŒØª ÙˆØ¶Ø¹ÛŒØª Ø¨Ø±Ù†Ø§Ù…Ù‡ ---
if 'gemini_model' not in st.session_state: st.session_state.gemini_model = None
if 'messages' not in st.session_state: st.session_state.messages = []
if 'vector_store' not in st.session_state: st.session_state.vector_store = None
if 'scraped_data' not in st.session_state: st.session_state.scraped_data = None
if 'html_content' not in st.session_state: st.session_state.html_content = None
if 'current_domain' not in st.session_state: st.session_state.current_domain = None
if 'scraper_code_to_approve' not in st.session_state: st.session_state.scraper_code_to_approve = None


# --- Ø¨Ø®Ø´ Ø¬Ø¯ÛŒØ¯: Ù…Ø¯ÛŒØ±ÛŒØª Ù‡ÙˆØ´Ù…Ù†Ø¯ Ú©Ù„ÛŒØ¯ API ---
st.sidebar.title("ØªÙ†Ø¸ÛŒÙ…Ø§Øª")
st.sidebar.header("Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ Ú©Ù„ÛŒØ¯ Gemini API")

def configure_api(api_key: str) -> bool:
    """Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ API Ø¨Ø§ Ú©Ù„ÛŒØ¯ Ø§Ø±Ø§Ø¦Ù‡ Ø´Ø¯Ù‡ Ùˆ Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†Ø¯Ù† ÙˆØ¶Ø¹ÛŒØª Ù…ÙˆÙÙ‚ÛŒØª."""
    if not api_key:
        return False
    try:
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel('gemini-2.5-pro')
        st.session_state.gemini_model = model
        return True
    except Exception as e:
        st.sidebar.error(f"Ú©Ù„ÛŒØ¯ API Ù†Ø§Ù…Ø¹ØªØ¨Ø± Ø§Ø³Øª: {e}")
        st.session_state.gemini_model = None
        return False

# Ø§ÙˆÙ„ÙˆÛŒØª Ø§ÙˆÙ„: ØªÙ„Ø§Ø´ Ø¨Ø±Ø§ÛŒ Ø®ÙˆØ§Ù†Ø¯Ù† Ú©Ù„ÛŒØ¯ Ø§Ø² st.secrets
if st.session_state.gemini_model is None:
    try:
        secrets_key = st.secrets.get("GEMINI_API_KEY")
        if secrets_key and configure_api(secrets_key):
            st.sidebar.success("Ú©Ù„ÛŒØ¯ API Ø§Ø² secrets.toml Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯. âœ…")
    except (FileNotFoundError, KeyError):
        # Ø§ÛŒÙ† Ø®Ø·Ø§ Ø·Ø¨ÛŒØ¹ÛŒ Ø§Ø³Øª Ø§Ú¯Ø± ÙØ§ÛŒÙ„ secrets ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯
        pass

# Ø§Ú¯Ø± Ú©Ù„ÛŒØ¯ Ø§Ø² secrets Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù†Ø´Ø¯ØŒ Ø¨Ù‡ Ú©Ø§Ø±Ø¨Ø± Ø§Ø¬Ø§Ø²Ù‡ ÙˆØ±ÙˆØ¯ Ù…ÛŒâ€ŒØ¯Ù‡ÛŒÙ…
if st.session_state.gemini_model is None:
    st.sidebar.info("Ú©Ù„ÛŒØ¯ÛŒ Ø¯Ø± ÙØ§ÛŒÙ„ secrets.toml ÛŒØ§ÙØª Ù†Ø´Ø¯. Ù„Ø·ÙØ§Ù‹ Ú©Ù„ÛŒØ¯ Ø®ÙˆØ¯ Ø±Ø§ Ø¯Ø± Ø²ÛŒØ± ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯.")
    user_api_key = st.sidebar.text_input(
        "Ú©Ù„ÛŒØ¯ API Gemini Ø®ÙˆØ¯ Ø±Ø§ Ø§ÛŒÙ†Ø¬Ø§ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯:",
        type="password",
        help="Ú©Ù„ÛŒØ¯ Ø´Ù…Ø§ ÙÙ‚Ø· Ø¯Ø± Ø§ÛŒÙ† Ø¬Ù„Ø³Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ùˆ Ø¬Ø§ÛŒÛŒ Ø°Ø®ÛŒØ±Ù‡ Ù†Ø®ÙˆØ§Ù‡Ø¯ Ø´Ø¯."
    )
    if st.sidebar.button("ØªØ§ÛŒÛŒØ¯ Ùˆ Ø°Ø®ÛŒØ±Ù‡ Ú©Ù„ÛŒØ¯"):
        if configure_api(user_api_key):
            st.sidebar.success("Ú©Ù„ÛŒØ¯ API Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª ØªØ§ÛŒÛŒØ¯ Ùˆ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯.")
            time.sleep(1)
            st.rerun()
        else:
            st.sidebar.warning("Ù„Ø·ÙØ§Ù‹ ÛŒÚ© Ú©Ù„ÛŒØ¯ Ù…Ø¹ØªØ¨Ø± ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯.")

# --- ØªÙˆØ§Ø¨Ø¹ Ù…Ø¯ÛŒØ±ÛŒØª Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡ Ø§Ø³Ú©Ø±Ù¾Ø±Ù‡Ø§ ---
def load_scrapers() -> dict:
    if not os.path.exists(SCRAPER_FILE): return {}
    try:
        with open(SCRAPER_FILE, 'r', encoding='utf-8') as f: return json.load(f)
    except (json.JSONDecodeError, FileNotFoundError): return {}

def save_scraper(domain: str, code: str):
    scrapers = load_scrapers()
    scrapers[domain] = code
    with open(SCRAPER_FILE, 'w', encoding='utf-8') as f:
        json.dump(scrapers, f, indent=4, ensure_ascii=False)

# --- ØªÙˆØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ Ø¨Ø±Ù†Ø§Ù…Ù‡ ---
def fetch_html_simple(url: str):
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        st.session_state.html_content = response.text
    except requests.exceptions.RequestException as e:
        st.error(f"[Requests] Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø§Ø² Ø³Ø§ÛŒØª: {e}")
        st.session_state.html_content = None

def fetch_html_advanced(url: str):
    if not SELENIUM_AVAILABLE:
        st.error("Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡ Selenium Ù†ØµØ¨ Ù†Ø´Ø¯Ù‡ Ø§Ø³Øª.")
        st.session_state.html_content = None
        return
    driver = None
    status = st.status("Ø¯Ø± Ø­Ø§Ù„ Ø§Ø¬Ø±Ø§ÛŒ Ù…Ø±ÙˆØ±Ú¯Ø± Ø®ÙˆØ¯Ú©Ø§Ø± (Selenium)...", expanded=True)
    try:
        chrome_options = Options()
        chrome_options.add_argument("--headless")
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        status.write("Ù†ØµØ¨ ÛŒØ§ Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø¯Ø±Ø§ÛŒÙˆØ± Ù…Ø±ÙˆØ±Ú¯Ø±...")
        service = ChromeService(ChromeDriverManager().install())
        status.write(f"Ø¨Ø§Ø² Ú©Ø±Ø¯Ù† Ø¢Ø¯Ø±Ø³: {url}")
        driver = webdriver.Chrome(service=service, options=chrome_options)
        driver.set_page_load_timeout(30)
        driver.get(url)
        status.write("Ù…Ù†ØªØ¸Ø± Ù…Ø§Ù†Ø¯Ù† Ø¨Ø±Ø§ÛŒ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø­ØªÙˆØ§ÛŒ Ø¯ÛŒÙ†Ø§Ù…ÛŒÚ©...")
        time.sleep(7)
        html_content = driver.page_source
        status.update(label="Ù…Ø±ÙˆØ±Ú¯Ø± Ø®ÙˆØ¯Ú©Ø§Ø± Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ú©Ø§Ø± Ø®ÙˆØ¯ Ø±Ø§ ØªÙ…Ø§Ù… Ú©Ø±Ø¯.", state="complete")
        st.session_state.html_content = html_content
    except Exception as e:
        st.error(f"[Selenium] Ø®Ø·Ø§ Ø¯Ø± Ø§Ø¬Ø±Ø§ÛŒ Ù…Ø±ÙˆØ±Ú¯Ø± Ø®ÙˆØ¯Ú©Ø§Ø±: {e}")
        status.update(label="Ø®Ø·Ø§ Ø¯Ø± Ø§Ø¬Ø±Ø§ÛŒ Selenium.", state="error")
        st.session_state.html_content = None
    finally:
        if driver:
            driver.quit()

def generate_scraper_with_gemini(html_content: str, site_url: str) -> str | None:
    generation_model = st.session_state.gemini_model
    soup = BeautifulSoup(html_content, 'lxml')
    body_content_sample = str(soup.body.prettify())[:100000] if soup.body else ""
    status = st.status("Ø¯Ø± Ø­Ø§Ù„ Ø§Ø¬Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ù…ØªØ®ØµØµØ§Ù†Ù‡ ØªÙˆØ³Ø· Gemini...", expanded=True)
    try:
        status.update(label="Ù…Ø±Ø­Ù„Ù‡ Û±: Ù†Ù‚Ø´Ù‡â€ŒØ¨Ø±Ø¯Ø§Ø±ÛŒ Ø§Ø² Ø³Ø§Ø®ØªØ§Ø± Ú©Ù„ÛŒ Ø³Ø§ÛŒØª...")
        prompt_step_1 = f"""Analyze the following HTML from {site_url}. Identify CSS selectors for the main containers that each hold a single news article. Provide your answer as a JSON object with a key "selectors" which is a list of strings. Example: {{"selectors": ["div.story-wrapper", "a.news-card"]}} \nHTML:\n```html\n{body_content_sample}\n```"""
        response_step_1 = generation_model.generate_content(prompt_step_1)
        selectors_json_str = response_step_1.text.strip().replace("```json", "").replace("```", "").strip()
        candidate_selectors = json.loads(selectors_json_str).get("selectors", [])
        if not candidate_selectors: return None
        status.write(f"Ù…Ø±Ø­Ù„Ù‡ Û±: Ø³Ù„Ú©ØªÙˆØ±Ù‡Ø§ÛŒ Ú©Ø§Ù†Ø¯ÛŒØ¯ Ù¾ÛŒØ¯Ø§ Ø´Ø¯Ù†Ø¯: `{'`, `'.join(candidate_selectors)}`")
        
        status.update(label="Ù…Ø±Ø­Ù„Ù‡ Û²: ØªØ­Ù„ÛŒÙ„ Ø¹Ù…ÛŒÙ‚ Ùˆ Ù…Ù‚Ø§ÛŒØ³Ù‡â€ŒØ§ÛŒ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§...")
        samples_html = ""
        found_articles = soup.select(", ".join(candidate_selectors), limit=5)
        if not found_articles: return None
        for article in found_articles: samples_html += f"<!-- Sample Article Container -->\n{article.prettify()}\n\n"
        prompt_step_2 = f"""Based on the HTML samples, determine reliable CSS selectors to extract title, link, and description. Provide analysis as a clean JSON object with keys: "best_article_selector", "title_selector", "link_selector", "description_selector". If the link is on the main container itself, use "self".\nHTML SAMPLES:\n```html\n{samples_html}\n```"""
        response_step_2 = generation_model.generate_content(prompt_step_2)
        analysis = json.loads(response_step_2.text.strip().replace("```json", "").replace("```", "").strip())
        status.write("Ù…Ø±Ø­Ù„Ù‡ Û²: ØªØ­Ù„ÛŒÙ„ Ø³Ø§Ø®ØªØ§Ø± Ø¯Ø§Ø®Ù„ÛŒ Ù…Ù‚Ø§Ù„Ø§Øª Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯.")

        status.update(label="Ù…Ø±Ø­Ù„Ù‡ Û³: ØªÙˆÙ„ÛŒØ¯ Ú©Ø¯ Ù†Ù‡Ø§ÛŒÛŒ...")
        prompt_step_3 = f"""Write a Python function `scrape_news(html_content)` using this analysis: `{json.dumps(analysis)}`. Base URL: `{site_url}`. CRITICAL: Must include `from bs4 import BeautifulSoup` and `from urllib.parse import urljoin`. Use `try-except Exception: continue`. Return ONLY Python code."""
        response_step_3 = generation_model.generate_content(prompt_step_3)
        final_code = response_step_3.text.strip().replace("```python", "").replace("```", "").strip()
        status.update(label="ØªØ­Ù„ÛŒÙ„ Ù…ØªØ®ØµØµØ§Ù†Ù‡ Ùˆ ØªÙˆÙ„ÛŒØ¯ Ú©Ø¯ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ù‡ Ù¾Ø§ÛŒØ§Ù† Ø±Ø³ÛŒØ¯!", state="complete")
        return final_code
    except Exception as e:
        status.update(label=f"Ø®Ø·Ø§ Ø¯Ø± ÙØ±Ø¢ÛŒÙ†Ø¯ ØªØ­Ù„ÛŒÙ„: {e}", state="error")
        st.error(f"Ø®Ø·Ø§ Ø¯Ø± ÙØ±Ø¢ÛŒÙ†Ø¯ ØªØ­Ù„ÛŒÙ„ Ú†Ù†Ø¯ Ù…Ø±Ø­Ù„Ù‡â€ŒØ§ÛŒ: {e}")
        return None

def execute_scraper(html_content: str, scraper_code: str) -> list | None:
    try:
        execution_scope = {}
        exec(scraper_code, execution_scope)
        scraper_function = execution_scope.get('scrape_news')
        if callable(scraper_function):
            return scraper_function(html_content)
        st.error("ØªØ§Ø¨Ø¹ `scrape_news` Ø¯Ø± Ú©Ø¯ ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯Ù‡ ÛŒØ§ÙØª Ù†Ø´Ø¯.")
        return None
    except Exception as e:
        st.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø§Ø¬Ø±Ø§ÛŒ Ú©Ø¯ Ø§Ø³ØªØ®Ø±Ø§Ø¬â€ŒÚ©Ù†Ù†Ø¯Ù‡: {e}")
        return None

def build_vector_store(scraped_data: list):
    st.session_state.vector_store = []
    if not scraped_data: return
    items_to_embed = [item for item in scraped_data if item.get('title') and item.get('link')]
    if not items_to_embed: return

    # --- Ø§ØµÙ„Ø§Ø­ Ú©Ù„ÛŒØ¯ÛŒ Ø¨Ø±Ø§ÛŒ Ø±ÙØ¹ Ø¨Ø§Ú¯ Ù„ÛŒÙ†Ú© ---
    # Ù„ÛŒÙ†Ú© Ø®Ø¨Ø± Ø±Ø§ Ø¨Ù‡ Ù…ØªÙ†ÛŒ Ú©Ù‡ embed Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ø§Ø¶Ø§ÙÙ‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… ØªØ§ Ø¯Ø± Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ù†Ø´ Ø¨Ø±Ø§ÛŒ Ø¬Ø³ØªØ¬Ùˆ Ùˆ Ù¾Ø§Ø³Ø®Ú¯ÙˆÛŒÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ø¨Ø§Ø´Ø¯.
    contents = [f"Ø¹Ù†ÙˆØ§Ù†: {item.get('title', '')}\nØªÙˆØ¶ÛŒØ­Ø§Øª: {item.get('description', '')}\nÙ…Ù†Ø¨Ø¹ (Ù„ÛŒÙ†Ú©): {item.get('link', '')}" for item in items_to_embed]
    
    with st.spinner(f"Ø¯Ø± Ø­Ø§Ù„ Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ù†Ø´ Ø¨Ø±Ø§ÛŒ Ú†Øªâ€ŒØ¨Ø§Øª (Ù¾Ø±Ø¯Ø§Ø²Ø´ {len(contents)} Ø®Ø¨Ø±)..."):
        try:
            BATCH_SIZE = 100
            all_embeddings = []
            for i in range(0, len(contents), BATCH_SIZE):
                batch_content = contents[i:i+BATCH_SIZE]
                embedding_response = genai.embed_content(model="models/text-embedding-004", content=batch_content, task_type="RETRIEVAL_DOCUMENT")
                all_embeddings.extend(embedding_response['embedding'])
                time.sleep(1) # Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ø®Ø·Ø§Ù‡Ø§ÛŒ rate limit
            
            for content, vector in zip(contents, all_embeddings):
                st.session_state.vector_store.append({"content": content, "vector": np.array(vector)})
            st.success(f"Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ù†Ø´ Ú†Øªâ€ŒØ¨Ø§Øª Ø¨Ø§ **{len(st.session_state.vector_store)}** Ø³Ù†Ø¯ Ø¢Ù…Ø§Ø¯Ù‡ Ø´Ø¯.")
        except Exception as e:
            st.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø³Ø§Ø®Øª embedding: {e}")

def find_relevant_context(query: str, top_k: int = 7) -> str:
    if not st.session_state.vector_store: return ""
    try:
        query_embedding = genai.embed_content(model="models/text-embedding-004", content=query, task_type="RETRIEVAL_QUERY")['embedding']
        query_vector = np.array(query_embedding)
        vectors = np.array([item['vector'] for item in st.session_state.vector_store])
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´Ø¨Ø§Ù‡Øª Ú©Ø³ÛŒÙ†ÙˆØ³ÛŒ
        dot_products = np.dot(vectors, query_vector)
        norms = np.linalg.norm(vectors, axis=1) * np.linalg.norm(query_vector)
        similarities = dot_products / norms
        
        top_k_indices = np.argsort(similarities)[-top_k:][::-1]
        context = "### Ù…ØªÙˆÙ† Ù…Ø±ØªØ¨Ø· Ø§Ø² Ø§Ø®Ø¨Ø§Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯Ù‡:\n\n"
        for index in top_k_indices:
            context += st.session_state.vector_store[index]['content'] + "\n---\n"
        return context
    except Exception as e:
        st.error(f"Ø®Ø·Ø§ Ø¯Ø± Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ù…ØªÙ† Ù…Ø±ØªØ¨Ø·: {e}")
        return ""

# --- Ø±Ø§Ø¨Ø· Ú©Ø§Ø±Ø¨Ø±ÛŒ Ø§ØµÙ„ÛŒ Streamlit ---
st.title("ØªØ­Ù„ÛŒÙ„Ú¯Ø± Ù‡ÙˆØ´Ù…Ù†Ø¯ Ùˆ Ù¾Ø§ÛŒØ¯Ø§Ø± Ø§Ø®Ø¨Ø§Ø± ğŸ¤–ğŸ“")

# Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¶Ø¹ÛŒØª Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ API Ù‚Ø¨Ù„ Ø§Ø² Ù†Ù…Ø§ÛŒØ´ Ø±Ø§Ø¨Ø· Ú©Ø§Ø±Ø¨Ø±ÛŒ Ø§ØµÙ„ÛŒ
if st.session_state.gemini_model is None:
    st.info("ğŸ’¡ Ø¨Ø±Ø§ÛŒ Ø´Ø±ÙˆØ¹ØŒ Ù„Ø·ÙØ§Ù‹ Ø§Ø² Ù…Ù†ÙˆÛŒ Ú©Ù†Ø§Ø±ÛŒ (Sidebar) Ú©Ù„ÛŒØ¯ Gemini API Ø®ÙˆØ¯ Ø±Ø§ ÙˆØ§Ø±Ø¯ Ùˆ ØªØ§ÛŒÛŒØ¯ Ú©Ù†ÛŒØ¯.")
    st.warning("Ø§Ú¯Ø± ÙØ§ÛŒÙ„ `.streamlit/secrets.toml` Ø±Ø§ Ø¨Ø§ Ú©Ù„ÛŒØ¯ Ø®ÙˆØ¯ Ø§ÛŒØ¬Ø§Ø¯ Ú©Ù†ÛŒØ¯ØŒ Ø¨Ø±Ù†Ø§Ù…Ù‡ Ø¨Ù‡ ØµÙˆØ±Øª Ø®ÙˆØ¯Ú©Ø§Ø± Ø§Ø² Ø¢Ù† Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø®ÙˆØ§Ù‡Ø¯ Ú©Ø±Ø¯.")
else:
    st.markdown("Ø§ÛŒÙ† Ø¨Ø±Ù†Ø§Ù…Ù‡ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ø³Ø§ÛŒØªØŒ ÛŒÚ© Ø§Ø³Ú©Ø±Ù¾Ø± Ù…Ø®ØµÙˆØµ Ø¨Ø§ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ù…ÛŒâ€ŒØ³Ø§Ø²Ø¯ Ùˆ Ø¢Ù† Ø±Ø§ Ø°Ø®ÛŒØ±Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ ØªØ§ Ø¯Ø± Ø§Ø³ØªÙØ§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø¹Ø¯ÛŒ Ø³Ø±ÛŒØ¹â€ŒØªØ± Ùˆ Ø§Ø±Ø²Ø§Ù†â€ŒØªØ± Ø¹Ù…Ù„ Ú©Ù†Ø¯.")
    
    col1, col2 = st.columns([1, 2])

    with col1:
        st.subheader("Û±. Ù…Ø¯ÛŒØ±ÛŒØª Ø§Ø³ØªØ®Ø±Ø§Ø¬")
        url = st.text_input("Ø¢Ø¯Ø±Ø³ ÙˆØ¨â€ŒØ³Ø§ÛŒØª:", placeholder="https://www.varzesh3.com/")
        fetch_method = st.radio("Ø±ÙˆØ´ Ø¯Ø±ÛŒØ§ÙØª:", ('Ø³Ø§Ø¯Ù‡ (Requests)', 'Ù¾ÛŒØ´Ø±ÙØªÙ‡ (Selenium)'), index=1, horizontal=True)
        force_regenerate = st.checkbox("Ø³Ø§Ø®Øª Ù…Ø¬Ø¯Ø¯ Ø§Ø³Ú©Ø±Ù¾Ø± (Ù†Ø§Ø¯ÛŒØ¯Ù‡ Ú¯Ø±ÙØªÙ† Ú©Ø¯ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù‡)")

        if st.button("ğŸš€ Ø´Ø±ÙˆØ¹ Ù¾Ø±Ø¯Ø§Ø²Ø´", type="primary", use_container_width=True):
            if url:
                # Ø±ÛŒØ³Øª Ú©Ø±Ø¯Ù† ÙˆØ¶Ø¹ÛŒØª Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¬Ø¯ÛŒØ¯
                st.session_state.messages, st.session_state.vector_store = [], None
                st.session_state.scraped_data, st.session_state.scraper_code_to_approve = None, None
                
                domain = urlparse(url).netloc
                st.session_state.current_domain = domain
                scrapers = load_scrapers()
                fetch_html_advanced(url) if fetch_method == 'Ù¾ÛŒØ´Ø±ÙØªÙ‡ (Selenium)' else fetch_html_simple(url)

                if st.session_state.html_content:
                    st.success("âœ”ï¸ Ù…Ø­ØªÙˆØ§ÛŒ HTML Ø¯Ø±ÛŒØ§ÙØª Ø´Ø¯.")
                    if domain in scrapers and not force_regenerate:
                        st.info(f"âœ… Ø§Ø³Ú©Ø±Ù¾Ø± Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ `{domain}` Ø§Ø¬Ø±Ø§ Ù…ÛŒâ€ŒØ´ÙˆØ¯...")
                        scraper_code = scrapers[domain]
                        st.session_state.scraped_data = execute_scraper(st.session_state.html_content, scraper_code)
                    else:
                        st.info(f"Ø¯Ø± Ø­Ø§Ù„ Ø³Ø§Ø®Øª Ø§Ø³Ú©Ø±Ù¾Ø± Ø¬Ø¯ÛŒØ¯ Ø¨Ø±Ø§ÛŒ `{domain}`...")
                        generated_code = generate_scraper_with_gemini(st.session_state.html_content, url)
                        if generated_code:
                            st.session_state.scraper_code_to_approve = generated_code
                        else:
                            st.error("ØªÙˆÙ„ÛŒØ¯ Ú©Ø¯ Ù†Ø§Ù…ÙˆÙÙ‚ Ø¨ÙˆØ¯. Ù…Ù…Ú©Ù† Ø§Ø³Øª Ø³Ø§Ø®ØªØ§Ø± Ø³Ø§ÛŒØª Ù¾ÛŒÚ†ÛŒØ¯Ù‡ Ø¨Ø§Ø´Ø¯ ÛŒØ§ Ø®Ø·Ø§ÛŒÛŒ Ø¯Ø± API Ø±Ø® Ø¯Ø§Ø¯Ù‡ Ø¨Ø§Ø´Ø¯.")
                else:
                    st.error("Ø¯Ø±ÛŒØ§ÙØª HTML Ù†Ø§Ù…ÙˆÙÙ‚ Ø¨ÙˆØ¯.")
            else:
                st.warning("Ù„Ø·ÙØ§Ù‹ Ø¢Ø¯Ø±Ø³ Ø±Ø§ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯.")

        if st.session_state.get('scraper_code_to_approve'):
            st.subheader("Û². ØªØ§ÛŒÛŒØ¯ Ùˆ Ø§ØµÙ„Ø§Ø­ Ø§Ø³Ú©Ø±Ù¾Ø±")
            edited_code = st.text_area("Ú©Ø¯ Ø§Ø³Ú©Ø±Ù¾Ø±:", value=st.session_state.scraper_code_to_approve, height=300, key="editor")
            c1, c2 = st.columns(2)
            if c1.button("âœ… ØªØ³Øª Ùˆ Ø§Ø¬Ø±Ø§", use_container_width=True):
                with st.spinner("Ø¯Ø± Ø­Ø§Ù„ ØªØ³Øª Ú©Ø¯..."):
                    st.session_state.scraped_data = execute_scraper(st.session_state.html_content, edited_code)
            if c2.button("ğŸ’¾ Ø°Ø®ÛŒØ±Ù‡ Ùˆ Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ", type="primary", use_container_width=True, disabled=not bool(st.session_state.scraped_data)):
                save_scraper(st.session_state.current_domain, edited_code)
                st.success(f"Ú©Ø¯ Ø¨Ø±Ø§ÛŒ `{st.session_state.current_domain}` Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯.")
                build_vector_store(st.session_state.scraped_data)
                st.session_state.scraper_code_to_approve = None # Ù¾Ø§Ú© Ú©Ø±Ø¯Ù† Ú©Ø¯ Ø§Ø² Ø­Ø§Ù„Øª ØªØ§ÛŒÛŒØ¯
                st.rerun()

        if st.session_state.get('scraped_data') is not None:
            st.subheader("Ù†ØªØ§ÛŒØ¬ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯Ù‡")
            total_items = len(st.session_state.scraped_data)
            st.success(f"ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ø§Ø®Ø¨Ø§Ø± ÛŒØ§ÙØª Ø´Ø¯Ù‡: **{total_items}**")
            with st.expander(f"Ø¨Ø±Ø§ÛŒ Ù…Ø´Ø§Ù‡Ø¯Ù‡ {total_items} Ø®Ø¨Ø± Ú©Ù„ÛŒÚ© Ú©Ù†ÛŒØ¯"):
                st.dataframe(st.session_state.scraped_data)
            if not st.session_state.get('vector_store'):
                if st.button("ğŸ§  Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ø³Ø´ Ùˆ Ù¾Ø§Ø³Ø®", use_container_width=True):
                    build_vector_store(st.session_state.scraped_data)
                    st.rerun()

    with col2:
        st.subheader("Ù¾Ø±Ø³Ø´ Ùˆ Ù¾Ø§Ø³Ø® Ø§Ø² Ù…Ø­ØªÙˆØ§")
        if st.session_state.get('vector_store'):
            for msg in st.session_state.messages:
                with st.chat_message(msg["role"]):
                    st.markdown(msg["content"], unsafe_allow_html=True)
            
            if prompt := st.chat_input("Ø³Ø¤Ø§Ù„ Ø®ÙˆØ¯ Ø±Ø§ Ø¯Ø± Ù…ÙˆØ±Ø¯ Ø§Ø®Ø¨Ø§Ø± Ø¨Ù¾Ø±Ø³ÛŒØ¯..."):
                st.session_state.messages.append({"role": "user", "content": prompt})
                with st.chat_message("user"):
                    st.markdown(prompt, unsafe_allow_html=True)
                
                with st.chat_message("assistant"):
                    message_placeholder = st.empty()
                    with st.spinner("Ø¯Ø± Ø­Ø§Ù„ ØªØ­Ù„ÛŒÙ„ Ø³ÙˆØ§Ù„ Ùˆ ÛŒØ§ÙØªÙ† Ù¾Ø§Ø³Ø®..."):
                        generation_model = st.session_state.gemini_model
                        
                        intent_prompt = f"""Classify the user's query into 'retrieval' or 'aggregation'.
                        - 'retrieval': Asks about specific content (e.g., "What did the coach say?").
                        - 'aggregation': Asks for a list, summary, or count (e.g., "List all headlines", "How many news items?").
                        Query: "{prompt}" -> Category:"""
                        intent_response = generation_model.generate_content(intent_prompt)
                        intent = intent_response.text.strip().lower()

                        full_response = ""
                        if "aggregation" in intent:
                            message_placeholder.info("Ø¯Ø±Ø®ÙˆØ§Ø³Øª ØªØ¬Ù…ÛŒØ¹ÛŒ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯... Ø¯Ø± Ø­Ø§Ù„ Ù¾Ø±Ø¯Ø§Ø²Ø´ ØªÙ…Ø§Ù… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§.")
                            all_data = [f"Ø¹Ù†ÙˆØ§Ù†: {item.get('title', 'Ø¨Ø¯ÙˆÙ† Ø¹Ù†ÙˆØ§Ù†')}\nÙ„ÛŒÙ†Ú©: {item.get('link', 'Ø¨Ø¯ÙˆÙ† Ù„ÛŒÙ†Ú©')}" for item in st.session_state.scraped_data]
                            context = "\n".join(all_data)
                            total_count = len(all_data)
                            final_prompt = f"""You are an assistant. Answer the user's question in Persian based ONLY on the provided list of data. Create a clear, structured, and categorized summary of the news.
                            Data: Total articles: {total_count}\n{context}
                            User's Question: {prompt}"""
                        else: # retrieval
                            message_placeholder.info("Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø¬Ø³ØªØ¬ÙˆÛŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø®Ø§Øµ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯...")
                            context = find_relevant_context(prompt)
                            if not context.strip() or "###" not in context:
                                full_response = "Ù…ØªØ§Ø³ÙØ§Ù†Ù‡ Ù†ØªÙˆØ§Ù†Ø³ØªÙ… Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…Ø±ØªØ¨Ø·ÛŒ Ø¯Ø± Ø§Ø®Ø¨Ø§Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯Ù‡ Ù¾ÛŒØ¯Ø§ Ú©Ù†Ù…."
                            else:
                                final_prompt = f"""You are a helpful AI assistant. Answer the user's question in Persian based ONLY on the provided context.
                                **CRITICAL: When you mention a news item, you MUST provide its source link, which is included in the context. Format the link as a clickable Markdown link, like this: [Ù…ØªÙ† Ù„ÛŒÙ†Ú©](URL).**
                                If the answer isn't in the context, say so clearly.

                                Context:
                                {context}
                                
                                Question: {prompt}
                                """
                        
                        if not full_response: # Ø§Ú¯Ø± Ù¾Ø§Ø³Ø® Ø§Ø² Ù‚Ø¨Ù„ Ù…Ø´Ø®Øµ Ù†Ø´Ø¯Ù‡ Ø¨ÙˆØ¯
                            response = generation_model.generate_content(final_prompt)
                            full_response = response.text

                    message_placeholder.markdown(full_response, unsafe_allow_html=True)
                st.session_state.messages.append({"role": "assistant", "content": full_response})
        else:
            st.info("Ø§Ø¨ØªØ¯Ø§ Ø§Ø·Ù„Ø§Ø¹Ø§Øª ÛŒÚ© Ø³Ø§ÛŒØª Ø±Ø§ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ùˆ Ø³Ù¾Ø³ Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ø³Ø´ Ùˆ Ù¾Ø§Ø³Ø® Ø¢Ù…Ø§Ø¯Ù‡ Ú©Ù†ÛŒØ¯ ØªØ§ Ø§ÛŒÙ† Ø¨Ø®Ø´ ÙØ¹Ø§Ù„ Ø´ÙˆØ¯.")